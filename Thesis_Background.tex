%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                      %
%     File: Thesis_Background.tex                                      %
%     Tex Master: Thesis.tex                                           %
%                                                                      %
%     Author: Guilherme Sousa                                          %
%                                                                      %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background}
\label{chapter:background}

In this chapter the tools used to approach the problematic of this project will be described firstly. A theoretical model of the plane dynamics will be discussed, and implementation details will be provided in chapter \ref{chapter:implementation}. The control strategy used will then follow, giving an overview of the feedback linearisation approach, as well as its limitations, namely its sensitivity to inversion errors and external interferences. An attempt to solve these limitation will be made, suggesting some solutions and finally describing the chosen methodology for this case.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Feedback Linearisation}
\label{section:background/NLI}
% Aqui vamos nos!

Feedback linearisation, also known as dynamic inversion, is an approach based on the idea of algebraically transforming a non-linear system into a linear one, from which linear control laws can be used to control the resulting system. Unlike Jacobian linearisation, that assumes linearity of the system around an equilibrium value, feedback linearisation implements a feedback loop that cancels non-linearities of the given system \cite{Slotine+Li}. Given a nonlinear system 

\begin{equation}
\dot{x} = f(x,u)
\label{eq:nonlinear_system}
\end{equation}

one must find both a state transformation $z=z(x)$ and an input transformation $\upsilon = \upsilon(x,u)$ in order for the transformed system to be a linear and time-invariant system $\dot{z} = Az+B\upsilon$. This process is called Input-Output linearisation. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1\textwidth]{Figures/NLI}
  \caption[Feedback linearisation example]{Feedback linearisation example \cite{Slotine+Li}}
  \label{fig:nli}
\end{figure}

To better understand Input-Output linearisation, some mathematical tools must firstly be described. One of these tools is the Lie derivative, that represents the gradient of a given scalar function $h(x)$ projected along a given vector function $f(x)$. Such a Lie derivative is therefore represented as

\begin{equation}
L_fh(x) = \nabla h(x)f(x) = \Sigma^n_{i=1}\dfrac{\partial h(x)}{\partial x_i}f_i(x)
\end{equation}

A Lie derivative can also be applied multiple times, resulting in a $k^{th}$ order Lie derivative

\begin{equation}
L^k_fh(x)=L_f\left(L^{k-1}_fh(x)\right)=\nabla \left(L^{k-1}_fh(x)\right)f(x) \quad \text{with} \quad L^0_fh(x)=h(x)
\end{equation}

The second operator from Lie algebra to be introduced here, quite important in Input-Output linearisation is the Lie bracket, and these are defined as, given two vector fields $f$ and $g$,

\begin{equation}
[f,g] = ad_f g = \nabla g f - \nabla f g
\end{equation}

Once again Lie brackets can be defined recursively as

\begin{equation}
ad_f^ig=[f, ad_f^{i-1}g] \quad \text{with} \quad ad_f^0g=g
\end{equation}

Finally, the concept of \emph{diffeomorphism} must be described to fully understand this control technique. According to \cite{Slotine+Li}, it is a generalisation of the concept of coordinate transformation, and is formally stated as:

\emph{A function $\phi: R^n\rightarrow R^n$, defined in a region $\Omega$, is called a diffeomorphism if it is smooth and its inverse $\phi^{-1}$ exists and is smooth \cite{Slotine+Li}}. 
In Input-Output linearisation, a \emph{diffeomorphism} must be found to transform a non-linear system into a linear one.
\subsection{SISO Systems}
\label{section:background/SISO_NLI}

Although an aircraft is always considered as a MIMO system, a description of this control concept will firstly be provided for a general SISO case, before generalising to a MIMO case. Nonlinear systems that can be represented as affine systems $\dot{x}=f(x)+g(x)u$ will be discussed in this section. From \cite{Slotine+Li}, a definition of Input-State linearisation is to find, for a nonlinear system of relative degree $n$, a \emph{diffeomorphism} $\phi:\Omega \rightarrow R^n$ and nonlinear control law
\begin{equation}
u=\alpha(x) + \beta(x)\upsilon
\label{eq:nli_control_law}
\end{equation}

such that the new state variables $z=\phi(x)$ and \emph{pseudo-input} $\upsilon$ satisfy the linear time invariant system $\dot{z} = Az+B\upsilon$, where the following equations are satisfied

\begin{equation}
	\begin{cases}
		\dot{z_i}=z_{i+1} & if\quad i<n\\
		\dot{z_n}=\upsilon & if\quad i=n
	\end{cases}
	\label{eq:SISO_state}
\end{equation}

In order to find such a function and control law, one must find a state $z_1$ such that 
\begin{subequations}
	\begin{equation}
		\nabla z_1 ad_f^ig=0 \qquad i=0, ..., n-2
	\end{equation}
	\begin{equation}
		\nabla z_1 ad_f^{n-1}g\neq 0
	\end{equation}
\end{subequations}

The remaining states are then obtained from $z(x) = [z_1 \quad L_fz_1 \quad ... \quad L_f^{n-1}z_1]^T$ and the input transformation from

\begin{subequations}
	\begin{equation}
		\alpha (x) = - \dfrac{L_f^nz_1}{L_gL_f^{n-1}z_1}
	\end{equation}
	\begin{equation}
		\beta (x) = \dfrac{1}{L_gL_f^{n-1}z_1}
	\end{equation}
\end{subequations}

\subsection{MIMO Systems}
\label{section:background/MIMO_NLI}

These concepts can also be extended to MIMO systems in a similar manner, by differentiating the outputs until the inputs explicitly appear. This time however, there are individual relative degrees per output. The sum of these relative degrees is called total degree $r$ and must satisfy $r\leq n$, $n$ being the order of the system. Note however, that in the case of $r<n$, a part of the system dynamics is rendered unobservable in the input-output linearisation, as the closed loop dynamics will have a lower order $r$ than the whole dynamics of the system $n$. These internal dynamics must therefore be studied to establish the effectiveness of the feedback linearisation. For the case where the internal dynamics is stable, the input-output linearisation method can then be used. In the case of unstable internal dynamics however, these would lead to both undesirable and uncontrollable phenomena, in which case a different control design is needed.

Given a MIMO system 
\begin{gather}
\dot{x} = f(x) + G(x)u\\
y=h(x)
\end{gather}
the $\phi$ functions are defined as $\phi^i_j(x)=L^{j-1}_fh_i(
x)$ and satisfy a condition similar to \ref{eq:SISO_state} 
\begin{equation}
\dot{\phi}^i_1(x)=\phi^i_2,...,\dot{\phi}^i_{r_i-1}=\phi^i_{r_i} \quad \text{and} \quad \dot{\phi}^i_{r_i}=L_f^{r_i}h_i(x)+\sum^m_{j=1}L_{g_j}L^{r_i-1}_fh_i(x)u_j
\end{equation}
As this equation holds for $1<i<p$, $p$ being the number of outputs of the system, giving an expression for the pseudo control input $\upsilon$
\begin{equation}
\upsilon = 
\begin{bmatrix}
\dot{\phi}^1_{r_1}\\
\vdots\\
\dot{\phi}^p_{r_p}\\
\end{bmatrix}
\end{equation}

It is also interesting to note the resulting system is not only linear, but also completely decoupled, as each new pseudo input only affects one output.

\section{Improving the Feedback Linearisation Method}
\label{section:background/limitations}
Although feedback control answers to many of the problems of linear controllers such as PID and LQR, it has a some downsides. Indeed, being a model based controller, it requires the system model to be quite accurately known, and parameter uncertainties can lead to undesired responses. Let $\upsilon$ be the vector of pseudo inputs, for a second order $\ddot{x} = f(x,\dot{x},u)$, if it is input-output linearisable and the exact system is known, then it comes that, from \cite{YANG+LIN_Adaptive_Flight_Control}
\begin{equation}
\ddot{x}=\upsilon
\end{equation}
However, even for simpler models, a real system is difficult to accurately describe, and an approximation $\hat{f}$ of $f$ is usually chosen, resulting in $\upsilon=\hat{f}(x,\dot{x},u)$. Therefore, defining a dynamic inversion error, $\Delta(x,\dot{x},u)=f(x,\dot{x},u)-\hat{f}(x,\dot{x},u)$, comes that, as stated in \cite{YANG+LIN_Adaptive_Flight_Control}
\begin{equation}
\ddot{x}=\upsilon + \Delta(x,\dot{x},u)
\label{eq:system+error}
\end{equation}
There can be many causes for a dynamic inversion error to be present, as not only modelling incertitudes can lead to larger errors, but also external interference (wind gusts) and actuator faults. The goal of this section will be to present the reader with some solutions to minimize this error. 

\subsection{Fuzzy Logic Systems}
\label{section:background/fuzzy_logic}

Fuzzy logic systems are based on the paradigm of continuous levels of truth between 0 and 1, rather than the usual discrete true or false levels of truth. Using these continuous truth values, a set of IF-ELSE rules is chosen. The goal of this type of controller is to duplicate the way a pilot would respond to a given flight situation. These rules can base their control input on variables such as roll, angle of attack and sideslip \cite{Comparison_IntelligentSys}. This control method is based on three main parts. The first one if \textit{fuzzification}, that consists of converting the plant outputs to fuzzy logic values. \textit{Rule-based inference}, using the previously mentioned set of rules, a fuzzyfied control input is computed, which then goes through the \textit{defuzzyfication} part of the control algorithm. 
Fuzzy systems have some advantages: its linguistic interpretation of human knowledge facilitate the interpretation of results, and the knowledge base can be improved through addition of new rules. However, this method comes with some disadvantages. First of all, the set of rules that will improve the control of the aircraft is entirely dependent on expert knowledge, thus resulting in an empiric set of rules. This also means the method has no capability of generalization, as the control is only applied to specific cases, of the form "if the error is greater than a given value, apply this compensation to control input". It is also not robust to topological changes of the system \cite{Neuro_fuzzy_survey}. For these reasons this method will not be further studied and implemented in this work.


\subsection{Neural Networks}
\label{section:background/NN}

Artificial Neural Networks (NN) are a biologically inspired simulation of the brain nervous system. They are composed of simulated neurons and synapses. The sum of the inputs of a neuron are summed and the result is fed into an activation function, which then return a bounded value, either between -1 and 1 or 0 and 1. The inputs and outputs of a neuron are multiplied by a weight, representing the synapses between neurons. We get the following equation for one neuron
\begin{equation}
y(x_1,x_2,...,x_n)=f((\sum ^n_{i=1} w_i x_i)+b)
\end{equation}
where $b$ is a bias term and $f$ a given activation function. Neurons are then set in three different types of layers: the input, the output and the hidden layers. These are connected between each other, and the hidden part can be composed of several layers. The weights can then be tuned based on experience, making it suitable for intelligent learning. To tune these weights learning algorithms are used. There are two types of learning algorithms: batch and online training. It is called training to the process of iteratively reaching the ideal set of weights that will minimise the error between the output of the neural network and that of a given unknown function.

Batch training is mostly based on a property of Neural Networks that, according to the Universal Approximation Theorem showed in 1989 by George Cybenko, a feed forward neural network can approximate any continuous function. The network is trained by providing input and output pairs. The weights are found in order to approximate the output of the neural network to that of the original function. Once these weights are computed, the network can then be used to compute outputs from inputs that were not part of the initial knowledge of input-output pairs.

Although this learning method approach is used in many of NN applications, it cannot however be easily used to improve an existing feedback linearisation  controller, as it requires an \textit{a priori} knowledge of the modelling error for each given input.

Online training, contrary to batch training, can deal with dynamically changing environments. This training paradigm is not based on an \textit{a priori} knowledge set, and instead relies on update laws that compute the new weights after each control iteration. It is this method that will be retained to adaptively correct the inversion error from the feedback linearisation. Training a network online, however, is less trivial when compared to batch training. An algorithm named backpropagation will be presented to provide a way of training a network online. A description on the implementation of the neural network and how it will be used to improve the feedback linearisation controller will be given on chapter \ref{chapter:implementation}.


%Backprop
The backpropagation algorithm is one of the most common NN training algorithm, commonly used in batch training, to update the weights of each layer of a network, in order to minimize a cost function. However, research has been done in order to use backpropagation techniques to create online adaptive and augmented control such as \cite{online_adaptiveNN}, \cite{UAV_adaptive}, \cite{UAV_adaptive2}. Indeed, it is also possible to use such algorithms to continuously update the weights of a network as the data is generated. Backpropagation can only be implemented if the activation function of the NN is differentiable. 

The backpropagation algorithm can be described by a two phase cycle, Propagation and Weight update. During this last phase the gradient descent algorithm is used to optimize a cost function. As such, a cost function $E$ must be chosen, and must be a function of the outputs of the neural network. A commonly used error function is 
\begin{equation}
E=\frac{1}{2} (y_d-y)^2
\end{equation}

Where $y_d$ is the desired output of the network and $y$ is the actual output of the network. The $\frac{1}{2}$ factor is added to be cancelled when differentiating. For a given neuron $i$, the sum of its inputs $sum_i$ is given by
\begin{equation}\label{eq:def_sum}
sum_i = \sum ^n_{k=1} w_{ki} x_k
\end{equation}
Where $w_{ki}$ is the weight of the $k^{th}$ input of neuron $i$, and $x_k$ are the outputs of the neurons form the previous layer. Given an activation function $f(x)$, we define the output of neuron $i$ as 
\begin{equation}\label{eq:NN_output}
x_i = f(sum_i)
\end{equation}

In order to use the gradient descent algorithm, the derivative of the cost function with respect to a given weight $w_{ij}$ must be computed. To do so, the chain rule is used as
\begin{equation}\label{eq:gradient}
\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial x_j}\frac{\partial x_j}{\partial sum_j}\frac{\partial sum_j}{\partial w_{ij}}
\end{equation}
These three factors can now be easily computed. The last two factors are independent of weather $x_j$ is a hidden or output layer and will for this reason be calculated firstly. From \ref{eq:def_sum} and \ref{eq:NN_output} comes that
\begin{equation}
\frac{\partial x_j}{\partial sum_j} = \frac{\partial f(sum_j)}{\partial sum_j}
\end{equation}
For an output layer, where values may be unbounded, linear output functions may be used, for which the equation above has a trivial solution. For output layers of classification neural networks or for hidden layers in general, logistic or tangent sigmoid functions can be used instead. For the first case let $f(x)=\frac{1}{1+e^{-x}}$, then

\begin{equation}
\frac{\partial f(sum_j)}{\partial sum_j}=f(sum_j)(1-f(sum_j))
\end{equation}
For the case of a tangent sigmoid activation function $f(x)=tanh(x)$ comes
\begin{equation}
\frac{\partial f(sum_j)}{\partial sum_j}=1-f^2(sum_j)
\end{equation}
For the term $\frac{\partial sum_j}{\partial w_{ij}}$, from \ref{eq:def_sum} this partial derivative is easily computed

\begin{equation}
\frac{\partial sum_j}{\partial w_{ij}} = \frac{\partial \left(\sum ^n_{k=1} w_{kj} x_k\right)}{\partial w_{ij}} = x_i
\end{equation}
For the output layer, the first term of equation \ref{eq:gradient} can easily be evaluated, as $x_j=y$ for such a case, giving
\begin{equation}
\frac{\partial E}{\partial x_j}=\frac{\partial (\frac{1}{2} (y_d-y)^2)}{\partial y}= y-y_d
\end{equation}
For input layers however, this derivative is less obvious to estimate. One can consider that the error function $E$ is a function of the inputs of all neurons that take $x_j$ as input. Let $S$ be such a set of inputs, from the total derivative to $x_j$ comes that
\begin{equation}
\frac{\partial E}{\partial x_j} = \sum _{s\in S}\left( \frac{\partial E}{\partial x_s}\frac{\partial x_s}{\partial sum_s}\frac{\partial sum_s}{\partial x_j}\right)=\sum _{s\in S}\left( \frac{\partial E}{\partial x_s}\frac{\partial f(sum_s)}{\partial sum_s}w_{js}\right)
\end{equation}
Note that the term $ \frac{\partial E}{\partial x_s} $ represent the all the derivatives of the errors with respect to the outputs of the next layer of neurons. Taking into account that the last layer is easily evaluated without this method, the derivatives of all hidden neurons can be computed. Knowing the value of \ref{eq:gradient}, an update law for the weights can now be stated from the gradient descent algorithm
\begin{equation}\label{eq:update_law}
\Delta w_{ij} = - \alpha \frac{\partial E}{\partial w_{ij}}
\end{equation}
Where $\alpha$ is a learning coefficient, and its choice is crucial to assure a fast convergence of the solution. Indeed, while a exceedingly large learning coefficient can cause this algorithm to miss the minimum error, a small learning coefficient will also increase the convergence rate. The $-1$ factor is added in the equation above to converge to a local minima, removing it would make this algorithm maximize the cost function. A limitation of the backstepping algorithm that must also be taken into account is that it only guarantees local minima convergence. 
A detailed description on the connection of this algorithm with the control of the aircraft will be given on the next chapter.
